---
title       : Predictive Analytics in R
subtitle    : 
author      : David O'Brien <dunder.chief@gmail.com>
job         : 
framework   : revealjs        # {io2012, html5slides, shower, dzslides, ...}
revealjs    : {theme:      sky, 
               transition: concave} #cube, page, zoom, concave, linear, fade, default, none
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : monokai
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
---

# Predictive Analytics in R
### David O'Brien <dunder.chief@gmail.com>
### August 25, 2015

--- 

<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>

<script>
$('ul.incremental li').addClass('fragment')
$('ol.incremental li').addClass('fragment')
</script>

```{r, echo=FALSE}
# This code fixes incremental stuff with slidify and reveal.js
# https://gist.github.com/zross/edb5d1e42998da286e17
library(knitr)
opts_chunk$set(collapse=TRUE, echo=TRUE, 
               error=FALSE, message=FALSE, 
               warning=FALSE, cache=FALSE)

s0 <- knitr::knit_hooks$get('source')
o0 <- knitr::knit_hooks$get('output')
p0 <- knitr::knit_hooks$get('plot')

knitr::knit_hooks$set(
  list(
       
    source=function(x,options){
      if (is.null(options$class) & options$fig.num!=0) s0(x, options)
        
      else if (is.null(options$class) & options$fig.num==0)
       
      paste0("<pre><code class='r'>",paste0(x, collapse="\n"))  
      else
        
        paste0("<pre class='", options$class, "'><code class='r'>",paste0(x, collapse="\n"))          
      
    },
    output = function(x,options){
      if (is.null(options$class) & options$fig.num!=0) o0(x, options)
      else if (is.null(options$class))
        paste0('\n',x,'</code></pre>')
      else 
        #print(x)
        paste0('\n',x,'</code></pre>')
    },
    #    plot = function(x, options){
    #      print(p0(x, options))
    #    },
    chunk=function(x,options){
      
      if (is.null(options$class) & options$fig.num!=0){return(x)}
      else{
        if(grepl('</code></pre>',x)==1){return(x)}
        if(grepl('</code></pre>',x)!=1){return(paste0(x,'</code></pre>'))}
    }

      return(x)
    }
  )
)
```


What is Predictive Modeling?
-----------------------------------------------
<br> 

> 1. Given a set of **predictor variables (X)** 

> 2. Predict an **outcome (Y)**

<script> $('ol.incremental li').addClass('fragment')</script>

<aside class='notes'>

A simplified definition.

1. may not have an outcome Y
2. may want to know reasons behind __why__ X predicts Y

</aside>

---

Our Flower!
----------------------------------------------
<br>

<img src='assets/img/Iris_versicolor_nomeas.jpg' height='400'>



---

What kind of iris is this?
---------------------------------

<img src='assets/img/iris.png' width="700">


<br>

<img src='assets/img/Iris_versicolor_meas.jpg' height='300' class='fragment'>

```{r, echo=FALSE, results='asis'}
library(knitr)
library(AppliedPredictiveModeling)
library(MASS)
makeShow <- function() {slidify('index.Rmd'); browseURL('index.html');}
frag_it <- function(xx) {
  sub('<table>', '<table class="fragment">', xx)
}
example_row <- 55
ex_iris1 <- iris[example_row, ]
ex_iris1$Species <- '???'
iris_names <- paste(gsub('\\.', ' ', colnames(iris)), 
                    c('[X1]','[X2]','[X3]','[X4]','[Y]'), sep='\n')
out <- kable(ex_iris1, row.names=FALSE, col.names=iris_names, align='c', 
             format='html')
#cat(sub('<table>', '<table class="fragment">', out))
```

---

Our guess: 
---------------------------------------------------------------------

```{r, echo=FALSE, results='asis'}
kable(ex_iris1, row.names=FALSE, col.names=iris_names, align='c', format='html')
```


<img src='assets/img/down1.png' height='75' style='border: none; box-shadow: none;' class='fragment'>


.fragment <img src='assets/img/LDA_eq.png' height='75'>


<img src='assets/img/down1.png' height='75' style='border: none; box-shadow: none;' class='fragment'>

```{r echo=FALSE, results='asis'}
fit <- lda(Species ~ ., iris[-example_row, ])
pred <- t(predict(fit, iris[example_row, ])$posterior)
colnames(pred) <- 'Probablity'
out <- kable(round(pred, 3), format='html')
cat(sub('<table>', '<table class="fragment">', out))
```

<p style="color:red" class="fragment">Versicolor!</p>

<script>
$('ul.incremental li').addClass('fragment')
$('ol.incremental li').addClass('fragment')
</script>

--- 

How do we estimate these parameters: 
-----------------------------

<br>
<img src='assets/img/LDA_eq.png' height='100'>
<br>
```{r, echo=FALSE, results='asis'}
ex_iris2 <- rbind(iris[1:3, ], iris[51:53, ], iris[101:103, ])
out <- kable(data.frame(ex_iris2), row.names=FALSE, col.names=iris_names,
      align='c', format='html')
frag_it(out)
```

--- 


Implementation in R: 
------------------------------------------
<br>
```{r}
library(MASS)
trainset <- iris[-example_row, ] 
fit.lda <- lda(Species ~ ., data=trainset, prior=c(1/3, 1/3, 1/3)) 
pred <- predict(fit.lda, newdata=iris[example_row, ])
round(pred$posterior, 3)
```

<br> 

.fragment __Data Inputs:__ <br> formula, data.frame, matrix, or seperate X & Y objects 

<aside class='notes'>

Since most of the predictive modeling packages are written by different people,
they often have different option names/ input structure

</aside>

<script>
$('ul.incremental li').addClass('fragment')
$('ol.incremental li').addClass('fragment')
</script>



--- 


predict(fitObject, type = __???__)
---------------------------------------------

<br>

```{r, echo=FALSE, results='asis'}
out <- kable(
  data.frame(Model=c('gbm', 'mda', 'rpart', 'Weka', 'LogitBoost', 'lda'),
             Probability=c('"response"', '"posterior"', '"prob"', 
                           '"probability"', '"raw"','None needed' )), 
  format='html'
)
cat(sub('<table>', 
        '<table class="fragment" style="font-size: 40px; line-height: 50px;">', out))
```


<aside class='notes'>

There is some standardization, such as the predict function to test our model on a new datasets

</aside>

---

Typical flow for trying a new algorithm:
--------------------------------------------------------------

1. Find the package(s) and install
2. Find training function 
3. Split data into multiple train/test sets
4. Set up your data to fit the training model
    - Formula
    - Matrix
    - Data.frame
    - X, Y as seperate
5. Pre-process data
6. Look up tuning params
7. Write loops for model tuning / repeated cross-validation
8. Analyze results

<aside class='notes'>

Typical flow for base r

in caret all of this is contained in less than 5 lines of code

</aside>


---

Caret
-----------------------------

Website: <https://topepo.github.io/caret/index.html>
List of Models: <https://topepo.github.io/caret/modelList.html>

<br>

```{r, class="Fragment"}
options(stringsAsFactors=FALSE)
models <- read.csv('../caret_models.csv')
table(models$Type)
class_models <- subset(models, Type %in% c('Classification', 'Dual Use'),
                       select='method.Argument')
```

<script>
$('ul.incremental li').addClass('fragment')
$('ol.incremental li').addClass('fragment')
</script>

<aside class='notes'>

91 Machine learning packages

With all these dependencies, probably a few thousand packages in total???

</aside>

---

Train lots of models at once
---------------------------------------

<br>

```{r caretStart, class="fragment", eval=FALSE}
library(caret); library(doMC); registerDoMC(7)
myFits <- foreach(this.model = class_models) %do% {
  train(Species ~ ., 
        data=iris,
        method=this.model,
        preProcess='pca',
        trControl=trainControl(method='repeatedcv', number=5, repeats=7),
        tuneLength=5)
}
```

<script>
$('ul.incremental li').addClass('fragment')
$('ol.incremental li').addClass('fragment')
</script>

<br>

<aside class='notes'>

This will:
1. preprocess with PCA,
2. train with 5-fold cross validation, 7 repeats in parallel
3. will also optimize tuning parameters

Took XX minutes to run

Not all models worked because we have 3 categories


</aside>

---

<img src='assets/img/authors.png'>

<br>

<img src='assets/img/abstract_highlight.png'>

---

What else can caret do?
---------------------------------------

<br>

> - Data Splitting

> - Pre-processing

> - Feature Selection 

> - Model tuning / Resampling

> - Variable Importance

<script>
$('ul.incremental li').addClass('fragment')
$('ol.incremental li').addClass('fragment')
</script>

<aside class='notes'>

Easier to use than base R

Prevents common mistakes

</aside>

---

Data Splitting | Why?
---------------------------------------------------
__$$y = x^3$$__
```{r, echo=TRUE, eval=TRUE, fig.height=2}
y <- seq(1, 10, by=.1)
x <- seq(1, 10, by=.1)^3
par(mar=c(0,0,0,0))
plot(y ~ x, pch=16)
```

```{r, fig.height=2}
set.seed(1)
error <- rnorm(length(x), sd=2)
dat <- data.frame(X = x + error, Y = y + error)
par(mar=c(0,0,0,0))
plot(y ~ x, pch=16, col='gray')
points(Y ~ X, data=dat, pch='X', col='red2')
```

<aside class='notes'>

1. Create data where we know the optimal fit

2. Add some randomness to it

</aside>

---

```{r}
set.seed(1)
trainIndex <- createDataPartition(y=dat$Y, p=0.7, list=FALSE)

training <- dat[trainIndex, ]
test <- dat[-trainIndex, ]
```

---

In-sample (__training set__)   |   Out-of-sample (__test set__)

```{r, echo=FALSE, eval=TRUE, fig.height=2, warning=FALSE, message=FALSE}
polyPlot <- function(training, test, pol, color, return_err) {
  par(mar=c(0,0,0,0))
  plot(x, y, pch=16, col='gray',yaxt='n', xaxt='n', ann=FALSE)
  points(Y ~ X, data=test, pch='X', col=color)
  fit <- lm(Y ~ poly(X, pol, raw=TRUE), data=training)
  pred <- predict(fit, newdata=test)
  lines(training$X, fit$fitted.values, col='blue', lwd=3)
  error <- mean(abs(test$Y - pred))
  text(700, 3.5, paste('Error = ', round(error,2)), cex=2)
  text(675, 1.5, paste('Polynomial =', pol), cex=2)
  if(return_err) {return(error)} 
}
par(mfrow=c(1,2), mar=rep(0,4))
polyPlot(training, training, 1,  'red2', FALSE)
polyPlot(training, test, 1, 'green3', FALSE)

polyPlot(training, training, 3,  'red2', FALSE)
polyPlot(training, test, 3, 'green3', FALSE)

polyPlot(training, training, 20,  'red2', FALSE)
polyPlot(training, test, 20, 'green3', FALSE)

polyPlot(training, training, 50,  'red2', FALSE)
polyPlot(training, test, 50, 'green3', FALSE)
```

<aside class='notes'>

Error only decreases in training set

At polynomial = 50. Our model no longer works on new data

</aside>

---


<img src='assets/img/SL_bias_var_annot.png' height='600'>


<aside class='notes'>


</aside>

---


Data Splitting 
---------------------------------------------------

<br>

>  1. __Training set [70%]:__ <br> Train a model 100x with different tuning parameters <br><br>
>  2. __Cross-validation set [15%]:__ <br> Evaluate these 100 models <br><br>
>  3. __Test set [15%]:__ <br> Use final model __(only one!)__ to evaluate your the accuracy of your analysis

<script>
$('ul.incremental li').addClass('fragment')
$('ol.incremental li').addClass('fragment')
</script>

<aside class='notes'>

1. Most ML models have tuning parameters & we need to optimize these useing an out of sample dataset

2. This is our out of sample set for evaluating these params

3. In order to avoid overfitting due to tuning param selection, need a fresh test set

</aside>

---

Data Splitting
---------------------------------------------------


_Example:_

```{r}
library(caret)
trainIndex <- createDataPartition(iris$Species, p = .8,
                                  list = FALSE,
                                  times = 1)
irisTrain <- iris[ trainIndex, ]
irisTest  <- iris[-trainIndex, ]
```

<aside class='notes'>

__Why split data?__ To avoid overfitting our results

__Example:__ This is a good example of how caret make you do things the right way. I would normally just select random rows instead of breaking down into equal classes.

</aside>

---

Data Splitting (Time Series)
---------------------------------------------------

<br>

<img src='assets/img/Split_time.png'>

```{r, echo=FALSE, message=FALSE}
## Save to file in case I need to compile at presentation without internet 
library(quantmod)
# gold <- getSymbols('GLD', src='yahoo', from='1970-01-01', auto.assign=F)
# saveRDS(gold, '../gold_ts.rds')
gold <- readRDS('../gold_ts.rds')
```

```{r, eval=FALSE, class="fragment"}
library(quantmod)
gold <- getSymbols('GLD', src='yahoo', from='1970-01-01', auto.assign=FALSE)
```



<aside class='notes'>

Time series can't be split randomly because the slice we're predicting depends on the previous samples.

</aside>

---

```{r}
library(caret)
slices <- createTimeSlices(Cl(gold), initialWindow=1000, 
                           fixedWindow=TRUE, horizon=500, skip=500)
str(slices)
```



---

Data Splitting | Class imbalances
---------------------------------------------------

copy stuff from this webpage:
http://topepo.github.io/caret/sampling.html

---

Pre-processing
---------------------------------------------------

1. Get your data ready for training

2. Apply these training set transformations to test set

Example:

```{r}

```

<aside class='notes'>

What it is: Transforming predictor variables
Why: 

1. Center and scale so mean is 0 for all predictors with a STDEV of 1
2. Dimensionality reduction

why caret:
Makes you do it right by default, I kept doing it wrong at first.
Applies same parameters to the test set.

Go through example...

Imputation???

</aside>

---

Finding your model
---------------------------------------------------

```{r}

```

<aside class='notes'>

Caret has 192 models available how do we find these?

1. Website

2. code example
</aside>

---

Feature Selection
---------------------------------------------------

Selecting which subset of predictors will give us the best model

<aside class='notes'>

What is feature selection? 
__this is a subset of the features that we will need__


Why we need it:
__Can be challenging with many predictors & we can't try every possible model__

How to do it:
__method 1, 2, 3, etc.....__

</aside>

---

Model tuning / Resampling
----------------------------------------------------

Most models have at least one tuning parameter.

To optimize training parameters without overfitting our data,
we need to use resampling 

<aside class='notes'>

What
__Tune our model__

Why
__Avoid overfitting, get the best model__

How
__Examples__
k-nearest neighbor k is the tuning parameter
random forest: number of trees is a parameter
 
</aside>

---

Variable Importance
----------------------------------------------------

Rank predictors by usefulness

<aside class='notes'>

What
__A way to rank our predictors by how important they are to the model__

Why
__Help us remove predictors we don't want. And give us an idea about what causes our outcome variable__

How
__Examples__


</aside>

---

<!------------------H2O-------------------------------------->

h2o package: What and Why?
---------------------------------------------------------------------

- Java library utilizing hadoop for certain models

- Over multiple nodes

- http://h2o.ai/

<aside class='notes'>

Most of machine learning is subject to 'riduculously parallelization' because of optimization steps during training

But for really large data where params are already estimated, you
may want to parallelize a siCan be challenging with many predictors & we can'ngle model.

</aside>

---

List of models available with H2O
-------------------------------------------------------------

<aside class='notes'>

</aside>

---

Use case
-------------------------------------------------------------
<aside class='notes'>

</aside>

---

Setting up on Amazon
-------------------------------------------------------------

---

<!----------------GPU--------------------------------------->
GPU computing for machine learning in R
-------------------------------------------------------------

__Packages:__

- gputools
- rpud
- gmatrix
- Rth

<aside class='notes'>
A typical machine will have 4-8 cores

A GPU can have 1000 cores

All depend on CUDA infastructure (check this!)

OpenMP????
</aside>

---
<!-------------------THE END---------------------------------->

Places to Learn all about machine learning
---------------------------------------------------------------

- JHU datascience course
- Andrew Ng
- Statistical Learning
- Georgia Tech program

book1,2 & 3

---

References
---------------------------------------------------------------


























---

Why  Caret
----------------------------------------------------

A lot of models made by a lot of different people

- Syntactical minutea
- Baked in a lot of training control, tuning, and preprocessing (important b/c automatically applies to test set)
- Protects people from doing the wrong thing
- feature select the right way
- Table with a bunch of predict functions with different arguments



---


## Some models take formula, others take matrix, others take a data.frame

Data formating:

- formula
- data.frame
- matrix / vector
  
```{r}
# Examples of each
```

---

## What Caret is good for

Converts all of this to standard options

- Basically a wrapper for a lot of different models

---

## Models implemented

```{r}
#show the code to list all models
```

---

## Tuning parameters

- Easily parallelize tuning









