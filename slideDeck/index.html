<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Predictive Analytics in R</title>
  <meta name="description" content="">
  <meta name="author" content="David O'Brien <dunder.chief@gmail.com>">
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
  <link rel="stylesheet" href="libraries/frameworks/revealjs/css/reveal.min.css">
  <link rel="stylesheet" href="libraries/frameworks/revealjs/css/theme/sky.css" id="theme">
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/monokai.css" id="theme">
  <!--[if lt IE 9]>
  <script src="lib/js/html5shiv.js"></script>
  <![endif]-->  <link rel="stylesheet" href = "assets/css/ribbons.css">

</head>
<body>
  <div class="reveal">
    <div class="slides">
      <section class='' data-state='' id='slide-1'>
  <h1>Predictive Analytics in R</h1>
  <h3>David O&#39;Brien <a href="mailto:dunder.chief@gmail.com">dunder.chief@gmail.com</a></h3>

<h3>August 25, 2015</h3>

</section>
<section class='' data-state='' id='slide-2'>
  
  <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>

<script>
$('ul.incremental li').addClass('fragment')
$('ol.incremental li').addClass('fragment')
</script>

<h2>What is Predictive Modeling?</h2>

<p><br> </p>

<ol class = "build incremental">
<li><p>Given a set of <strong>predictor variables (X)</strong> </p></li>
<li><p>Predict an <strong>outcome (Y)</strong></p></li>
</ol>

<script> $('ol.incremental li').addClass('fragment')</script>

<p><aside class='notes'></p>

<p>A simplified definition.</p>

<ol>
<li>may not have an outcome Y</li>
<li>may want to know reasons behind <strong>why</strong> X predicts Y</li>
</ol>

<p></aside></p>

</section>
<section class='' data-state='' id='slide-3'>
  <h2>Our Flower!</h2>
  <p><br></p>

<p><img src='assets/img/Iris_versicolor_nomeas.jpg' height='400'></p>

</section>
<section class='' data-state='' id='slide-4'>
  <h2>What kind of iris is this?</h2>
  <p><img src='assets/img/iris.png' width="700"></p>

<p><br></p>

<p><img src='assets/img/Iris_versicolor_meas.jpg' height='300' class='fragment'></p>

</section>
<section class='' data-state='' id='slide-5'>
  <h2>Our guess: </h2>
  <table>
 <thead>
  <tr>
   <th style="text-align:center;"> Sepal Length
[X1] </th>
   <th style="text-align:center;"> Sepal Width
[X2] </th>
   <th style="text-align:center;"> Petal Length
[X3] </th>
   <th style="text-align:center;"> Petal Width
[X4] </th>
   <th style="text-align:center;"> Species
[Y] </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> 6.5 </td>
   <td style="text-align:center;"> 2.8 </td>
   <td style="text-align:center;"> 4.6 </td>
   <td style="text-align:center;"> 1.5 </td>
   <td style="text-align:center;"> ??? </td>
  </tr>
</tbody>
</table>

<table class="fragment">
 <thead>
  <tr>
   <th style="text-align:center;"> Sepal Length
[X1] </th>
   <th style="text-align:center;"> Sepal Width
[X2] </th>
   <th style="text-align:center;"> Petal Length
[X3] </th>
   <th style="text-align:center;"> Petal Width
[X4] </th>
   <th style="text-align:center;"> Species
[Y] </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> 5.1 </td>
   <td style="text-align:center;"> 3.5 </td>
   <td style="text-align:center;"> 1.4 </td>
   <td style="text-align:center;"> 0.2 </td>
   <td style="text-align:center;"> setosa </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 4.9 </td>
   <td style="text-align:center;"> 3.0 </td>
   <td style="text-align:center;"> 1.4 </td>
   <td style="text-align:center;"> 0.2 </td>
   <td style="text-align:center;"> setosa </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 4.7 </td>
   <td style="text-align:center;"> 3.2 </td>
   <td style="text-align:center;"> 1.3 </td>
   <td style="text-align:center;"> 0.2 </td>
   <td style="text-align:center;"> setosa </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 7.0 </td>
   <td style="text-align:center;"> 3.2 </td>
   <td style="text-align:center;"> 4.7 </td>
   <td style="text-align:center;"> 1.4 </td>
   <td style="text-align:center;"> versicolor </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 6.4 </td>
   <td style="text-align:center;"> 3.2 </td>
   <td style="text-align:center;"> 4.5 </td>
   <td style="text-align:center;"> 1.5 </td>
   <td style="text-align:center;"> versicolor </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 6.9 </td>
   <td style="text-align:center;"> 3.1 </td>
   <td style="text-align:center;"> 4.9 </td>
   <td style="text-align:center;"> 1.5 </td>
   <td style="text-align:center;"> versicolor </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 6.3 </td>
   <td style="text-align:center;"> 3.3 </td>
   <td style="text-align:center;"> 6.0 </td>
   <td style="text-align:center;"> 2.5 </td>
   <td style="text-align:center;"> virginica </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 5.8 </td>
   <td style="text-align:center;"> 2.7 </td>
   <td style="text-align:center;"> 5.1 </td>
   <td style="text-align:center;"> 1.9 </td>
   <td style="text-align:center;"> virginica </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 7.1 </td>
   <td style="text-align:center;"> 3.0 </td>
   <td style="text-align:center;"> 5.9 </td>
   <td style="text-align:center;"> 2.1 </td>
   <td style="text-align:center;"> virginica </td>
  </tr>
</tbody>
</table>

<p class='fragment'><img src='assets/img/LDA_eq.png' height='75'> </p>

<table class="fragment">
 <thead>
  <tr>
   <th style="text-align:left;">   </th>
   <th style="text-align:right;"> setosa </th>
   <th style="text-align:right;"> versicolor </th>
   <th style="text-align:right;"> virginica </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> Probablity </td>
   <td style="text-align:right;"> 0 </td>
   <td style="text-align:right;"> 0.995 </td>
   <td style="text-align:right;"> 0.005 </td>
  </tr>
</tbody>
</table>

<p style="color:red" class="fragment">Versicolor!</p>

<script>
$('ul.incremental li').addClass('fragment')
$('ol.incremental li').addClass('fragment')
</script>

</section>
<section class='' data-state='' id='slide-6'>
  <h2>Implementation in R: </h2>
  <p><br></p>

<pre><code class="r">library(MASS)
trainset &lt;- iris[-our_flower, ] 
fit.lda &lt;- lda(Species ~ ., data=trainset, prior=c(1/3, 1/3, 1/3)) 
pred &lt;- predict(fit.lda, newdata=iris[our_flower, ])
round(pred$posterior, 3)
</code></pre>

<pre><code>##    setosa versicolor virginica
## 55      0      0.995     0.005
</code></pre>

<p><br> </p>

<p><aside class='notes'></p>

<p>Since most of the predictive modeling packages are written by different people,
they often have different option names/ input structure</p>

<p></aside></p>

<script>
$('ul.incremental li').addClass('fragment')
$('ol.incremental li').addClass('fragment')
</script>

</section>
<section class='' data-state='' id='slide-7'>
  <h2>predict(fitObject, type = <strong>???</strong>)</h2>
  <p><br></p>

<table class="fragment" style="font-size: 40px; line-height: 50px;">
 <thead>
  <tr>
   <th style="text-align:left;"> Model </th>
   <th style="text-align:left;"> Probability </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> gbm </td>
   <td style="text-align:left;"> &quot;response&quot; </td>
  </tr>
  <tr>
   <td style="text-align:left;"> mda </td>
   <td style="text-align:left;"> &quot;posterior&quot; </td>
  </tr>
  <tr>
   <td style="text-align:left;"> rpart </td>
   <td style="text-align:left;"> &quot;prob&quot; </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Weka </td>
   <td style="text-align:left;"> &quot;probability&quot; </td>
  </tr>
  <tr>
   <td style="text-align:left;"> LogitBoost </td>
   <td style="text-align:left;"> &quot;raw&quot; </td>
  </tr>
  <tr>
   <td style="text-align:left;"> lda </td>
   <td style="text-align:left;"> None needed </td>
  </tr>
</tbody>
</table>

<p><aside class='notes'></p>

<p>There is some standardization, such as the predict function to test our model on a new datasets</p>

<p></aside></p>

</section>
<section class='' data-state='' id='slide-8'>
  <h2>Typical flow for trying a new algorithm:</h2>
  <ol>
<li>Find the package(s) and install</li>
<li>Find training function </li>
<li>Split data into multiple train/test sets</li>
<li>Set up your data to fit the training model

<ul>
<li>Formula</li>
<li>Matrix</li>
<li>Data.frame</li>
<li>X, Y as seperate objects</li>
</ul></li>
<li>Pre-process data</li>
<li>Look up tuning params</li>
<li>Write loops for model tuning / repeated cross-validation</li>
<li>Analyze results</li>
</ol>

<p><aside class='notes'></p>

<p>Typical flow for base r</p>

<p><strong>Data Inputs:</strong> <br> formula, data.frame, matrix, or seperate X &amp; Y objects 
in caret all of this is contained in less than 5 lines of code</p>

<p></aside></p>

</section>
<section class='' data-state='' id='slide-9'>
  <h2>Caret</h2>
  <p>Website: <a href="https://topepo.github.io/caret/index.html">https://topepo.github.io/caret/index.html</a></p>

<p>List of Models: <a href="https://topepo.github.io/caret/modelList.html">https://topepo.github.io/caret/modelList.html</a></p>

<p><br></p>

<pre><code class="r">options(stringsAsFactors=FALSE)
models &lt;- read.csv(&#39;../caret_models.csv&#39;)
table(models$Type)
</code></pre>

<pre><code>## 
## Classification       Dual Use     Regression 
##             74             73             45
</code></pre>

<pre><code class="r">class_models &lt;- subset(models, Type %in% c(&#39;Classification&#39;, &#39;Dual Use&#39;),
                       select=&#39;method.Argument&#39;)
</code></pre>

<script>
$('ul.incremental li').addClass('fragment')
$('ol.incremental li').addClass('fragment')
</script>

<p><aside class='notes'></p>

<p>Wrapper for 192 models</p>

<p>91 Machine learning packages</p>

<p>With all these dependencies, probably a few thousand packages in total???</p>

<p>Can add your own</p>

<p></aside></p>

</section>
<section class='' data-state='' id='slide-10'>
  <h2>Train lots of models at once</h2>
  <p><br></p>

<pre><code class="r">library(caret); library(doMC); registerDoMC(7)
myFits &lt;- foreach(this.model = class_models) %do% {
  train(Species ~ ., 
        data=iris,
        method=this.model,
        preProcess=&#39;pca&#39;,
        trControl=trainControl(method=&#39;repeatedcv&#39;, number=5, repeats=7),
        tuneLength=5)
}
</code></pre>

<script>
$('ul.incremental li').addClass('fragment')
$('ol.incremental li').addClass('fragment')
</script>

<p><br></p>

<p><aside class='notes'></p>

<p>This will:
1. preprocess with PCA,</p>

<ol>
<li><p>train with 5-fold cross validation, 7 repeats in parallel</p></li>
<li><p>will also optimize tuning parameters</p></li>
</ol>

<p>Took XX minutes to run</p>

<p>Not all models worked because we have 3 categories</p>

<p></aside></p>

</section>
<section class='' data-state='' id='slide-11'>
  
  <p><img src='assets/img/authors.png'></p>

<p><br></p>

<p><img src='assets/img/abstract_highlight.png'></p>

</section>
<section class='' data-state='' id='slide-12'>
  <h2>Machine Learning Basics</h2>
  <p><br>
<br></p>

<pre><code class="r">library(caret)
</code></pre>

<pre><code>## Loading required package: lattice
## Loading required package: ggplot2
</code></pre>

<pre><code class="r">trainIndex &lt;- createDataPartition(iris$Species, p = .8,
                                  list = FALSE,
                                  times = 1)
irisTrain &lt;- iris[ trainIndex, ]
irisTest  &lt;- iris[-trainIndex, ]
</code></pre>

<p><br></p>

<p><img src='assets/img/train_test.png'>
<aside class='notes'></p>

<p><strong>Why split data?</strong> To avoid overfitting our results</p>

<p><strong>Example:</strong> This is a good example of how caret make you do things the right way. I would normally just select random rows instead of breaking down into equal classes.</p>

<p></aside></p>

</section>
<section class='' data-state='' id='slide-13'>
  <h2>Cross-validation: Avoid overfitting</h2>
  <p><strong>\[y = x^3\]</strong></p>

<pre><code class="r">y &lt;- seq(2, 10, by=.05)
x &lt;- seq(2, 10, by=.05)^3
</code></pre>

<p><img src="assets/fig/unnamed-chunk-10-1.png" alt="plot of chunk unnamed-chunk-10"> </p>

<pre><code class="r">set.seed(1)
error &lt;- rnorm(length(x), sd=2)
dat &lt;- data.frame(X = x + error, Y = y + error)
</code></pre>

<p><img src="assets/fig/unnamed-chunk-12-1.png" alt="plot of chunk unnamed-chunk-12"> </p>

<p><aside class='notes'></p>

<ol>
<li><p>Create data where we know the optimal fit</p></li>
<li><p>Add some randomness to it</p></li>
</ol>

<p></aside></p>

</section>
<section class='' data-state='' id='slide-14'>
  <h2>Tuning polynomials</h2>
  <pre><code class="r">set.seed(100)
trainIndex &lt;- createDataPartition(y=dat$Y, p=0.5, list=FALSE)

training &lt;- dat[trainIndex, ]
test &lt;- dat[-trainIndex, ]
</code></pre>

<p><br> </p>

<pre><code class="r">fit &lt;- lm(Y ~ poly(X, 3, raw=TRUE), data=training)
pred.training &lt;- predict(fit, newdata=training)
pred.test &lt;- predict(fit, newdata=test)
</code></pre>

<p><br></p>

<p>\[y=\theta_3x^3 + \theta_2x^2 + \theta_1x + \theta_0\]</p>

<p><br></p>

<p class='fragment'><strong>Our true fit is:</strong> \(\theta_3=1\), \(\{\theta_2, \theta_1, \theta_0\}=0\)</p>

<p><aside class='notes'></p>

<p>We always fit on the training set</p>

<p>But we will compare analyzing this tuning parameter on both the training &amp; test sets</p>

<p></aside></p>

</section>
<section class='' data-state='' id='slide-15'>
  
  <p>In-sample (<strong>training set</strong>)   |   Out-of-sample (<strong>test set</strong>)</p>

<p><img src="assets/fig/unnamed-chunk-15-1.png" alt="plot of chunk unnamed-chunk-15"> <img src="assets/fig/unnamed-chunk-15-2.png" alt="plot of chunk unnamed-chunk-15"> <img src="assets/fig/unnamed-chunk-15-3.png" alt="plot of chunk unnamed-chunk-15"> <img src="assets/fig/unnamed-chunk-15-4.png" alt="plot of chunk unnamed-chunk-15"> </p>

<p><aside class='notes'></p>

<p>Error only decreases in training set</p>

<p>At polynomial = 50. Our model no longer works on new data</p>

<p></aside></p>

</section>
<section class='' data-state='' id='slide-16'>
  
  <p><img src='assets/img/SL_bias_var_annot.png' height='600'></p>

<p><aside class='notes'>
error: lower is better. grey line is our training error. Test is red</p>

<p>training error will always go down as the model gets more flexible</p>

<p>but we want to know how well it does on new data</p>

<p>test set is a better reflection of this</p>

<p></aside></p>

</section>
<section class='' data-state='' id='slide-17'>
  <h2>Parsimony / Occam&#39;s Razor</h2>
  <p><br>
The simplest model is usually the best. 
<br>
Only use least number of parameters that are necessary.</p>

</section>
<section class='' data-state='' id='slide-18'>
  <h2>Data Splitting </h2>
  <p><br></p>

<ol class = "build incremental">
<li><strong>Training set [70%]:</strong> <br> Train a model 100x with different tuning parameters <br><br></li>
<li><strong>Cross-validation set [15%]:</strong> <br> Evaluate these 100 models <br><br></li>
<li><strong>Test set [15%]:</strong> <br> Use final model <strong>(only one!)</strong> to evaluate your the accuracy of your analysis</li>
</ol>

<script>
$('ul.incremental li').addClass('fragment')
$('ol.incremental li').addClass('fragment')
</script>

<p><aside class='notes'></p>

<ol>
<li><p>Most ML models have tuning parameters &amp; we need to optimize these useing an out of sample dataset</p></li>
<li><p>This is our out of sample set for evaluating these params</p></li>
<li><p>In order to avoid overfitting due to tuning param selection, need a fresh test set
example of this is on prev slide</p></li>
</ol>

<p></aside></p>

</section>
<section class='' data-state='' id='slide-19'>
  <h2>30% of data on testing?!?</h2>
  <p><br></p>

<p><img src='assets/img/Performance_Rows.png'></p>

</section>
<section class='' data-state='' id='slide-20'>
  <h2>k-fold cross-validation</h2>
  <p><br>
<img src='assets/img/k-fold_CV.png'></p>

<p><aside class='notes'></p>

<p>in this case we have 5-fold cross-validation</p>

<p>Average the error for all 5 of these to pick the best model</p>

<p></aside></p>

</section>
<section class='' data-state='' id='slide-21'>
  <h2>caret: Basic Syntax</h2>
  <pre><code>train(Species ~ ., 
        data=iris,
        method=&#39;gbm&#39;,
        preProcess=&#39;knnImpute&#39;,
        trControl=trainControl(method=&#39;repeatedcv&#39;, number=5, repeats=7),
        tuneLength=5)
</code></pre>

<p><img src='assets/img/TrainAlgo.png' height='300'></p>

</section>
<section class='' data-state='' id='slide-22'>
  <h2>train():</h2>
  <ul class = "build incremental">
<li>method: <em>our machine learning algorithm (select from 192)</em></li>
<li>preProcess: </li>
</ul>

<table class="fragment">
 <thead>
  <tr>
   <th style="text-align:left;"> Train_Options </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> BoxCox </td>
  </tr>
  <tr>
   <td style="text-align:left;"> YeoJohnson </td>
  </tr>
  <tr>
   <td style="text-align:left;"> expoTrans </td>
  </tr>
  <tr>
   <td style="text-align:left;"> center </td>
  </tr>
  <tr>
   <td style="text-align:left;"> scale </td>
  </tr>
  <tr>
   <td style="text-align:left;"> range </td>
  </tr>
  <tr>
   <td style="text-align:left;"> knnImpute </td>
  </tr>
  <tr>
   <td style="text-align:left;"> bagImpute </td>
  </tr>
  <tr>
   <td style="text-align:left;"> medianImpute </td>
  </tr>
  <tr>
   <td style="text-align:left;"> pca </td>
  </tr>
  <tr>
   <td style="text-align:left;"> ica </td>
  </tr>
  <tr>
   <td style="text-align:left;"> spatialSign </td>
  </tr>
</tbody>
</table>

</section>
<section class='' data-state='' id='slide-23'>
  <h2>trainControl():</h2>
  <pre><code class="r">out &lt;- data.frame(Resampling_Method=c(&quot;boot&quot;, &quot;boot632&quot;, &quot;cv&quot;, &quot;repeatedcv&quot;, &quot;LOOCV&quot;, &quot;LGOCV&quot;, &quot;none&quot;, &quot;oob&quot;, &quot;adaptive_cv&quot;, &quot;adaptive_boot&quot;, &quot;adaptive_LGOCV&quot;))
kable(out, format=&#39;html&#39;)
</code></pre>

<table>
 <thead>
  <tr>
   <th style="text-align:left;"> Resampling_Method </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> boot </td>
  </tr>
  <tr>
   <td style="text-align:left;"> boot632 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> cv </td>
  </tr>
  <tr>
   <td style="text-align:left;"> repeatedcv </td>
  </tr>
  <tr>
   <td style="text-align:left;"> LOOCV </td>
  </tr>
  <tr>
   <td style="text-align:left;"> LGOCV </td>
  </tr>
  <tr>
   <td style="text-align:left;"> none </td>
  </tr>
  <tr>
   <td style="text-align:left;"> oob </td>
  </tr>
  <tr>
   <td style="text-align:left;"> adaptive_cv </td>
  </tr>
  <tr>
   <td style="text-align:left;"> adaptive_boot </td>
  </tr>
  <tr>
   <td style="text-align:left;"> adaptive_LGOCV </td>
  </tr>
</tbody>
</table>

</section>
<section class='' data-state='' id='slide-24'>
  <h2>Adding custom tuning params</h2>
  <pre><code class="r">gbmGrid &lt;-  expand.grid(interaction.depth = c(1, 5, 9),
                        n.trees = (1:30)*50,
                        shrinkage = 0.1,
                        n.minobsinnode = 20)
head(gbmGrid)
</code></pre>

<pre><code>##   interaction.depth n.trees shrinkage n.minobsinnode
## 1                 1      50       0.1             20
## 2                 5      50       0.1             20
## 3                 9      50       0.1             20
## 4                 1     100       0.1             20
## 5                 5     100       0.1             20
## 6                 9     100       0.1             20
</code></pre>

<pre><code>train(Species ~ ., 
        data=iris,
        method=&#39;gbm&#39;,
        preProcess=&#39;pca&#39;,
        trControl=trainControl(method=&#39;repeatedcv&#39;, number=5, repeats=7),
        tuneGrid = gbmGrid)
</code></pre>

</section>
<section class='' data-state='' id='slide-25'>
  <h2>Adaptive Resampling</h2>
  <p>Speed up the optimazion process</p>

<pre><code class="r">fitControl2 &lt;- trainControl(method = &quot;adaptive_cv&quot;,
                            number = 10,
                            repeats = 5,
                            ## Estimate class probabilities
                            classProbs = TRUE,
                            ## Evaluate performance using 
                            ## the following function
                            summaryFunction = twoClassSummary,
                            ## Adaptive resampling information:
                            adaptive = list(min = 10,
                                            alpha = 0.05,
                                            method = &quot;gls&quot;,
                                            complete = TRUE))
</code></pre>

<p>ref: <a href="http://arxiv.org/abs/1405.6974">http://arxiv.org/abs/1405.6974</a></p>

</section>
<section class='' data-state='' id='slide-26'>
  <h2>What else can caret do?</h2>
  <p><br></p>

<ul class = "build incremental">
<li><p>Data Splitting</p></li>
<li><p>Pre-processing</p></li>
<li><p>Feature Selection </p></li>
<li><p>Model tuning / Resampling</p></li>
<li><p>Variable Importance</p></li>
</ul>

<script>
$('ul.incremental li').addClass('fragment')
$('ol.incremental li').addClass('fragment')
</script>

<p><aside class='notes'></p>

<p>Easier to use than base R</p>

<p>Prevents common mistakes</p>

<p></aside></p>

</section>
<section class='' data-state='' id='slide-27'>
  <h2>Data Splitting (Time Series)</h2>
  <p><br></p>

<p><img src='assets/img/Split_time.png'></p>

<pre><code class="r">library(quantmod)
gold &lt;- getSymbols(&#39;GLD&#39;, src=&#39;yahoo&#39;, from=&#39;1970-01-01&#39;, auto.assign=FALSE)
</code></pre>

<p><aside class='notes'></p>

<p>Time series can&#39;t be split randomly because the slice we&#39;re predicting depends on the previous samples.</p>

<p></aside></p>

</section>
<section class='' data-state='' id='slide-28'>
  <h2>Time Series</h2>
  <pre><code class="r">library(caret)
slices &lt;- createTimeSlices(Cl(gold), initialWindow=1000, 
                           fixedWindow=TRUE, horizon=500, skip=500)
str(slices)
</code></pre>

<pre><code>## List of 2
##  $ train:List of 3
##   ..$ Training0001: int [1:1000] 1 2 3 4 5 6 7 8 9 10 ...
##   ..$ Training0502: int [1:1000] 502 503 504 505 506 507 508 509 510 511 ...
##   ..$ Training1003: int [1:1000] 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 ...
##  $ test :List of 3
##   ..$ Testing0001: int [1:500] 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 ...
##   ..$ Testing0502: int [1:500] 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 ...
##   ..$ Testing1003: int [1:500] 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 ...
</code></pre>

</section>
<section class='' data-state='' id='slide-29'>
  <h2>Data Splitting | Class imbalances</h2>
  <pre><code class="r">set.seed(2969)
imbal_train &lt;- twoClassSim(10000, intercept = -20, linearVars = 20)
imbal_test  &lt;- twoClassSim(10000, intercept = -20, linearVars = 20)
table(imbal_train$Class)
</code></pre>

<pre><code>## 
## Class1 Class2 
##   9411    589
</code></pre>

<p><a href="http://topepo.github.io/caret/sampling.html">http://topepo.github.io/caret/sampling.html</a></p>

</section>
<section class='' data-state='' id='slide-30'>
  <h2>Pre-processing</h2>
  <p><br></p>

</section>
<section class='' data-state='' id='slide-31'>
  <h2>Other functions</h2>
  <p><br></p>

<pre><code class="r">dummyVars()

nearZeroVar()

findCorrelation()

findLinearCombos()

classDist()
</code></pre>

<p><aside class='notes'></p>

<p>The list goes into </p>

<ol>
<li>Center and scale so mean is 0 for all predictors with a STDEV of 1</li>
<li>Dimensionality reduction</li>
</ol>

<p>why caret:
Makes you do it right by default, I kept doing it wrong at first.
Applies same parameters to the test set.</p>

<p>Go through example...</p>

<p>Imputation???</p>

<p></aside></p>

</section>
<section class='' data-state='' id='slide-32'>
  <h2>Variable Importance</h2>
  <p><br>
<br></p>

<pre><code class="r">varImp()
</code></pre>

<p><aside class='notes'></p>

<p>What
<strong>A way to rank our predictors by how important they are to the model</strong></p>

<p>Why
<strong>Help us remove predictors we don&#39;t want. And give us an idea about what causes our outcome variable</strong></p>

<p>How
<strong>Examples</strong></p>

<p></aside></p>

</section>
<section class='' data-state='' id='slide-33'>
  <h2>Feature Selection</h2>
  <p><img src='assets/img/rand_feat.png' height='600' class='fragment'></p>

<p><aside class='notes'></p>

<p>What is feature selection? 
<strong>this is a subset of the features that we will need</strong></p>

<p>Why we need it:
<strong>Can be challenging with many predictors &amp; we can&#39;t try every possible model</strong></p>

<p>How to do it:
<strong>method 1, 2, 3, etc.....</strong></p>

<p></aside></p>

</section>
<section class='' data-state='' id='slide-34'>
  
  <p>Recursive Feature Elimination:</p>

<pre><code class="r">rfe() 
rfeControl()
# &lt;img src=&#39;assets/img/RFE.png&#39;&gt;
</code></pre>

<p>Genetic Algorithms:</p>

<pre><code class="r">gafs()
</code></pre>

<pre><code>## Error in gafs.default(): promise already under evaluation: recursive default argument reference or earlier problems?
</code></pre>

<pre><code class="r">gafsControl()
</code></pre>

<pre><code>## $functions
## $functions$summary
## function (data, lev = NULL, model = NULL) 
## {
##     if (is.character(data$obs)) 
##         data$obs &lt;- factor(data$obs, levels = lev)
##     postResample(data[, &quot;pred&quot;], data[, &quot;obs&quot;])
## }
## &lt;environment: namespace:caret&gt;
## 
## $functions$fit
## function (x, y, first, last, ...) 
## train(x, y, ...)
## &lt;environment: namespace:caret&gt;
## 
## $functions$pred
## function (object, x) 
## {
##     tmp &lt;- predict(object, x)
##     if (object$modelType == &quot;Classification&quot; &amp; !is.null(object$modelInfo$prob)) {
##         out &lt;- cbind(data.frame(pred = tmp), as.data.frame(predict(object, 
##             x, type = &quot;prob&quot;)))
##     }
##     else out &lt;- tmp
##     out
## }
## &lt;environment: namespace:caret&gt;
## 
## $functions$rank
## function (object, x, y) 
## {
##     vimp &lt;- varImp(object, scale = FALSE)$importance
##     if (object$modelType == &quot;Regression&quot;) {
##         vimp &lt;- vimp[order(vimp[, 1], decreasing = TRUE), , drop = FALSE]
##     }
##     else {
##         if (all(levels(y) %in% colnames(vimp))) {
##             avImp &lt;- apply(vimp[, levels(y), drop = TRUE], 1, 
##                 mean)
##             vimp$Overall &lt;- avImp
##         }
##     }
##     vimp$var &lt;- rownames(vimp)
##     vimp
## }
## &lt;environment: namespace:caret&gt;
## 
## $functions$selectSize
## function (x, metric, maximize) 
## {
##     best &lt;- if (maximize) 
##         which.max(x[, metric])
##     else which.min(x[, metric])
##     min(x[best, &quot;Variables&quot;])
## }
## &lt;environment: namespace:caret&gt;
## 
## $functions$selectVar
## function (y, size) 
## {
##     finalImp &lt;- ddply(y[, c(&quot;Overall&quot;, &quot;var&quot;)], .(var), function(x) mean(x$Overall, 
##         na.rm = TRUE))
##     names(finalImp)[2] &lt;- &quot;Overall&quot;
##     finalImp &lt;- finalImp[order(finalImp$Overall, decreasing = TRUE), 
##         ]
##     as.character(finalImp$var[1:size])
## }
## &lt;environment: namespace:caret&gt;
## 
## 
## $method
## [1] &quot;repeatedcv&quot;
## 
## $metric
## NULL
## 
## $maximize
## NULL
## 
## $number
## [1] 10
## 
## $repeats
## [1] 1
## 
## $returnResamp
## [1] &quot;final&quot;
## 
## $verbose
## [1] FALSE
## 
## $p
## [1] 0.75
## 
## $index
## NULL
## 
## $indexOut
## NULL
## 
## $seeds
## NULL
## 
## $holdout
## [1] 0
## 
## $genParallel
## [1] FALSE
## 
## $allowParallel
## [1] TRUE
</code></pre>

<p>Univariate Filters:</p>

<pre><code class="r">gafs()
</code></pre>

<pre><code>## Error in gafs.default(): promise already under evaluation: recursive default argument reference or earlier problems?
</code></pre>

<pre><code class="r">gafsControl()
</code></pre>

<pre><code>## $functions
## $functions$summary
## function (data, lev = NULL, model = NULL) 
## {
##     if (is.character(data$obs)) 
##         data$obs &lt;- factor(data$obs, levels = lev)
##     postResample(data[, &quot;pred&quot;], data[, &quot;obs&quot;])
## }
## &lt;environment: namespace:caret&gt;
## 
## $functions$fit
## function (x, y, first, last, ...) 
## train(x, y, ...)
## &lt;environment: namespace:caret&gt;
## 
## $functions$pred
## function (object, x) 
## {
##     tmp &lt;- predict(object, x)
##     if (object$modelType == &quot;Classification&quot; &amp; !is.null(object$modelInfo$prob)) {
##         out &lt;- cbind(data.frame(pred = tmp), as.data.frame(predict(object, 
##             x, type = &quot;prob&quot;)))
##     }
##     else out &lt;- tmp
##     out
## }
## &lt;environment: namespace:caret&gt;
## 
## $functions$rank
## function (object, x, y) 
## {
##     vimp &lt;- varImp(object, scale = FALSE)$importance
##     if (object$modelType == &quot;Regression&quot;) {
##         vimp &lt;- vimp[order(vimp[, 1], decreasing = TRUE), , drop = FALSE]
##     }
##     else {
##         if (all(levels(y) %in% colnames(vimp))) {
##             avImp &lt;- apply(vimp[, levels(y), drop = TRUE], 1, 
##                 mean)
##             vimp$Overall &lt;- avImp
##         }
##     }
##     vimp$var &lt;- rownames(vimp)
##     vimp
## }
## &lt;environment: namespace:caret&gt;
## 
## $functions$selectSize
## function (x, metric, maximize) 
## {
##     best &lt;- if (maximize) 
##         which.max(x[, metric])
##     else which.min(x[, metric])
##     min(x[best, &quot;Variables&quot;])
## }
## &lt;environment: namespace:caret&gt;
## 
## $functions$selectVar
## function (y, size) 
## {
##     finalImp &lt;- ddply(y[, c(&quot;Overall&quot;, &quot;var&quot;)], .(var), function(x) mean(x$Overall, 
##         na.rm = TRUE))
##     names(finalImp)[2] &lt;- &quot;Overall&quot;
##     finalImp &lt;- finalImp[order(finalImp$Overall, decreasing = TRUE), 
##         ]
##     as.character(finalImp$var[1:size])
## }
## &lt;environment: namespace:caret&gt;
## 
## 
## $method
## [1] &quot;repeatedcv&quot;
## 
## $metric
## NULL
## 
## $maximize
## NULL
## 
## $number
## [1] 10
## 
## $repeats
## [1] 1
## 
## $returnResamp
## [1] &quot;final&quot;
## 
## $verbose
## [1] FALSE
## 
## $p
## [1] 0.75
## 
## $index
## NULL
## 
## $indexOut
## NULL
## 
## $seeds
## NULL
## 
## $holdout
## [1] 0
## 
## $genParallel
## [1] FALSE
## 
## $allowParallel
## [1] TRUE
</code></pre>

<p>Simalated Annealing:</p>

<pre><code class="r">safs()
</code></pre>

<pre><code>## Error in safs.default(): promise already under evaluation: recursive default argument reference or earlier problems?
</code></pre>

<pre><code class="r">safsControl()
</code></pre>

<pre><code>## $functions
## $functions$summary
## function (data, lev = NULL, model = NULL) 
## {
##     if (is.character(data$obs)) 
##         data$obs &lt;- factor(data$obs, levels = lev)
##     postResample(data[, &quot;pred&quot;], data[, &quot;obs&quot;])
## }
## &lt;environment: namespace:caret&gt;
## 
## $functions$fit
## function (x, y, first, last, ...) 
## train(x, y, ...)
## &lt;environment: namespace:caret&gt;
## 
## $functions$pred
## function (object, x) 
## {
##     tmp &lt;- predict(object, x)
##     if (object$modelType == &quot;Classification&quot; &amp; !is.null(object$modelInfo$prob)) {
##         out &lt;- cbind(data.frame(pred = tmp), as.data.frame(predict(object, 
##             x, type = &quot;prob&quot;)))
##     }
##     else out &lt;- tmp
##     out
## }
## &lt;environment: namespace:caret&gt;
## 
## $functions$rank
## function (object, x, y) 
## {
##     vimp &lt;- varImp(object, scale = FALSE)$importance
##     if (object$modelType == &quot;Regression&quot;) {
##         vimp &lt;- vimp[order(vimp[, 1], decreasing = TRUE), , drop = FALSE]
##     }
##     else {
##         if (all(levels(y) %in% colnames(vimp))) {
##             avImp &lt;- apply(vimp[, levels(y), drop = TRUE], 1, 
##                 mean)
##             vimp$Overall &lt;- avImp
##         }
##     }
##     vimp$var &lt;- rownames(vimp)
##     vimp
## }
## &lt;environment: namespace:caret&gt;
## 
## $functions$selectSize
## function (x, metric, maximize) 
## {
##     best &lt;- if (maximize) 
##         which.max(x[, metric])
##     else which.min(x[, metric])
##     min(x[best, &quot;Variables&quot;])
## }
## &lt;environment: namespace:caret&gt;
## 
## $functions$selectVar
## function (y, size) 
## {
##     finalImp &lt;- ddply(y[, c(&quot;Overall&quot;, &quot;var&quot;)], .(var), function(x) mean(x$Overall, 
##         na.rm = TRUE))
##     names(finalImp)[2] &lt;- &quot;Overall&quot;
##     finalImp &lt;- finalImp[order(finalImp$Overall, decreasing = TRUE), 
##         ]
##     as.character(finalImp$var[1:size])
## }
## &lt;environment: namespace:caret&gt;
## 
## 
## $method
## [1] &quot;repeatedcv&quot;
## 
## $metric
## NULL
## 
## $maximize
## NULL
## 
## $number
## [1] 10
## 
## $repeats
## [1] 1
## 
## $returnResamp
## [1] &quot;final&quot;
## 
## $verbose
## [1] FALSE
## 
## $p
## [1] 0.75
## 
## $index
## NULL
## 
## $indexOut
## NULL
## 
## $seeds
## NULL
## 
## $holdout
## [1] 0
## 
## $improve
## [1] Inf
## 
## $allowParallel
## [1] TRUE
</code></pre>

</section>
<section class='' data-state='' id='slide-35'>
  
  <!------------------H2O-------------------------------------->

<h2>h2o package:</h2>

<p><a href="http://h2o.ai/">http://h2o.ai/</a></p>

<ul>
<li><p>Open Source Java library</p></li>
<li><p>Runs Single model over multiple nodes</p></li>
<li><p>Hadoop &amp; Spark</p></li>
<li><p>EC2 / Azure / Compute ENgine</p></li>
<li><p>R, Scala, Python, Web Browser, REST API</p></li>
<li><p>Run locally</p></li>
</ul>

<p><aside class='notes'></p>

<p>Most of machine learning is subject to &#39;riduculously parallelization&#39; because of optimization steps during training</p>

<p>But for really large data where params are already estimated, you
may want to parallelize a siCan be challenging with many predictors &amp; we can&#39;ngle model.</p>

<p></aside></p>

</section>
<section class='' data-state='' id='slide-36'>
  <h2>Models available with H2O</h2>
  <p><br></p>

<ul>
<li>K-Means</li>
<li>GLM</li>
<li>DRF</li>
<li>Na√Øve Bayes</li>
<li>PCA</li>
<li>GBM</li>
<li>Deep Learning</li>
</ul>

<p><aside class='notes'></p>

<p></aside></p>

</section>
<section class='' data-state='' id='slide-37'>
  
  <pre><code class="r">h2o.deeplearning(
   x, y, training_frame, model_id = &quot;&quot;,
   overwrite_with_best_model, 
   validation_frame, checkpoint,
   autoencoder = FALSE, 
   use_all_factor_levels = TRUE,
   activation = c(&quot;Rectifier&quot;, &quot;Tanh&quot;, &quot;TanhWithDropout&quot;,
   &quot;RectifierWithDropout&quot;, &quot;Maxout&quot;, &quot;MaxoutWithDropout&quot;), 
   hidden = c(200, 200), 
   epochs = 10, 
   train_samples_per_iteration = -2, 
   seed, 
   adaptive_rate=TRUE, 
   rho = 0.99, 
   epsilon = 1e-08, 
   rate = 0.005,
   rate_annealing = 1e-06, 
   rate_decay = 1, 
   momentum_start=0,
   momentum_ramp = 1e+06, 
   momentum_stable = 0,
   nesterov_accelerated_gradient = TRUE, 
   input_dropout_ratio=0, 
   hidden_dropout_ratios, 
   l1 = 0, l2 = 0, max_w2 = Inf,
   initial_weight_distribution=c(&quot;UniformAdaptive&quot;,
                                 &quot;Uniform&quot;,&quot;Normal&quot;),
   initial_weight_scale = 1, 
   loss = c(&quot;Automatic&quot;, &quot;CrossEntropy&quot;, &quot;MeanSquare&quot;, 
            &quot;Absolute&quot;, &quot;Huber&quot;), 
   distribution = c(&quot;AUTO&quot;, &quot;gaussian&quot;, &quot;bernoulli&quot;, 
                    &quot;multinomial&quot;, &quot;poisson&quot;, &quot;gamma&quot;, 
                    &quot;tweedie&quot;, &quot;laplace&quot;,&quot;huber&quot;), 
   tweedie_power = 1.5, 
   score_interval = 5, 
   score_training_samples,
   score_validation_samples, 
   score_duty_cycle, 
   classification_stop,
   regression_stop, 
   quiet_mode, 
   max_confusion_matrix_size, 
   max_hit_ratio_k,
   balance_classes = FALSE, 
   class_sampling_factors, 
   max_after_balance_size,
   score_validation_sampling, 
   diagnostics, 
   variable_importances, 
   fast_mode, 
   ignore_const_cols,  
   force_load_balance, 
   replicate_training_data, 
   single_node_mode, 
   shuffle_training_data, 
   sparse, col_major,
   average_activation, 
   sparsity_beta, 
   max_categorical_features,
   reproducible = FALSE, 
   export_weights_and_biases = FALSE,
   offset_column = NULL, 
   weights_column = NULL, 
   nfolds = 0,
   fold_column = NULL, 
   fold_assignment = c(&quot;AUTO&quot;, &quot;Random&quot;, &quot;Modulo&quot;),
   keep_cross_validation_predictions = FALSE, ...)
</code></pre>

</section>
<section class='' data-state='' id='slide-38'>
  <h2>Use case</h2>
  <pre><code class="r">library(h2o)
# Initialize h2o with nthreads (default is 2)
localH2O &lt;- h2o.init(nthreads = 4)
# Convert our datasets
iris.train.h2o &lt;- as.h2o(iris.train, localH2O)
iris.test.h2o &lt;- as.h2o(iris.test, localH2O)

# Run the model
model = h2o.deeplearning(x = colnames(iris)[-ncol(iris)],
                         y = &quot;Species&quot;,
                         training_frame = iris.train.h2o,
                         activation = &quot;Tanh&quot;,
                         hidden = c(10, 10, 10),
                         epochs = 10000)

# Check performance of test set
performance = h2o.performance(model = model, data=iris.test.h2o)
</code></pre>

<p><aside class='notes'></p>

<p></aside></p>

</section>
<section class='' data-state='' id='slide-39'>
  
  <!----------------GPU--------------------------------------->

<h2>GPU computing for machine learning in R</h2>

<p><strong>Packages:</strong></p>

<ul>
<li>gputools</li>
<li>rpud</li>
</ul>

<p><aside class='notes'>
A typical machine will have 4-8 cores</p>

<p>A GPU can have 1000 cores</p>

<p>All depend on CUDA infastructure (check this!)</p>

<p>OpenMP????
</aside></p>

</section>
<section class='' data-state='' id='slide-40'>
  <h2>gputools</h2>
  <pre><code class="r">gpuGLM()
gpuLM()
gpuHclust(gpuDist())
</code></pre>

<p><aside class='notes'></p>

<p>Many more mathmatic functions</p>

<p></aside></p>

</section>
<section class='' data-state='' id='slide-41'>
  <h2>rpud</h2>
  <p><a href="http://www.r-tutor.com/gpu-computing">http://www.r-tutor.com/gpu-computing</a></p>

<pre><code class="r">rpuHclust()
rvbm()
rhierLinearModel()
rpusvm()
</code></pre>

<p><aside class='notes'></p>

<p>rhierLinearModel() = Hierarchical Linear Model
rvbm() = Bayesian Classification with Gaussian Process</p>

<p></aside></p>

</section>
<section class='' data-state='' id='slide-42'>
  
  <!-------------------THE END---------------------------------->

<h2>Places to Learn all about machine learning</h2>

<ul>
<li>Andrew Ng, Coursera/Stanford   <a href="https://www.coursera.org/learn/machine-learning">https://www.coursera.org/learn/machine-learning</a></li>
<li>Trevor Hastie, Rob Tibirashi Statistical Learning  <a href="http://online.stanford.edu/course/statistical-learning-winter-2014">http://online.stanford.edu/course/statistical-learning-winter-2014</a></li>
<li>JHU Practical Machine Learning   <a href="https://www.coursera.org/course/predmachlearn">https://www.coursera.org/course/predmachlearn</a></li>
<li>Georgia Tech / Udacity M.S. in comp. sci.  <br> <a href="http://www.omscs.gatech.edu/">http://www.omscs.gatech.edu/</a></li>
</ul>

</section>
<section class='' data-state='' id='slide-43'>
  <h2>References</h2>
  <p><br></p>

<p><img src='assets/img/ESL.jpg' height='400'> <img src='assets/img/ISL.jpg' height='400'> <img src='assets/img/APM.png' height='400'></p>

</section>
    </div>
  </div>
</body>
  <script src="libraries/frameworks/revealjs/lib/js/head.min.js"></script>
  <script src="libraries/frameworks/revealjs/js/reveal.min.js"></script>
  <script>
  // Full list of configuration options available here:
  // https://github.com/hakimel/reveal.js#configuration
  Reveal.initialize({
    controls: true,
    progress: true,
    history: true,
    center: true,
    theme: Reveal.getQueryHash().theme || 'sky', 
    transition: Reveal.getQueryHash().transition || 'default', 
    dependencies: [
    // Cross-browser shim that fully implements classList -
    // https://github.com/eligrey/classList.js/
      { src: 'libraries/frameworks/revealjs/lib/js/classList.js', condition: function() { return !document.body.classList;}},
      // Zoom in and out with Alt+click
      { src: 'libraries/frameworks/revealjs/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
      // Speaker notes
      { src: 'libraries/frameworks/revealjs/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
      // Remote control your reveal.js presentation using a touch device
      //{ src: 'libraries/frameworks/revealjs/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
      ]
  });
  </script>  <!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script> -->
<script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
<script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<!-- DONE LOADING HIGHLIGHTER JS FILES -->
 

</html>